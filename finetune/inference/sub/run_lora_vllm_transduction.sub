#!/bin/bash
#SBATCH -J run_lora_vllm_beam # Job name
#SBATCH -o run_lora_vllm_beam_%j.out                  # output file (%j expands to jobID)
#SBATCH -e run_lora_vllm_beam_%j.err                  # error log file (%j expands to jobID)
#SBATCH -N 1                                 # Total number of nodes requested
#SBATCH -n 1                                 # Total number of cores requested
#SBATCH --get-user-env                       # retrieve the users login environment
#SBATCH --mem=30000                            # server memory requested (per node)
#SBATCH -t 300:00:00                           # Time limit (hh:mm:ss)
#SBATCH --gres=gpu:3090:1

# >>> conda initialize >>>
declare CONDA_PATH=/share/apps/anaconda3/2021.05/
# !! Contents within this block are managed by 'conda init' !!
__conda_setup="$($CONDA_PATH'/bin/conda' 'shell.bash' 'hook' 2> /dev/null)"
if [ $? -eq 0 ]; then
    eval "$__conda_setup"
else
    if [ -f "$CONDA_PATH/etc/profile.d/conda.sh" ]; then
        . "$CONDA_PATH/etc/profile.d/conda.sh"
    else
        export PATH="$CONDA_PATH/bin:$PATH"
    fi
fi
unset __conda_setup
conda deactivate
conda deactivate
conda activate llama_factory

cd ..
# python vllm_inference_beamsearch.py --cache_dir cache/transduction_direct_best --model_name transduction_direct_best --if_transduction T --if_induction F
python vllm_inference_transduction.py --cache_dir cache/transduction_and_induction --model_name transduction_and_induction
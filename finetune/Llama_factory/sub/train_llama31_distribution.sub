#!/bin/bash
#SBATCH -J train_llama31_direct_distribution # Job name
#SBATCH -o train_llama31_direct_distribution_%j.out                  # output file (%j expands to jobID)
#SBATCH -e train_llama31_direct_distribution_%j.err                  # error log file (%j expands to jobID)
#SBATCH -N 1                                 # Total number of nodes requested
#SBATCH -n 8                                 # Total number of cores requested
#SBATCH --get-user-env                       # retrieve the users login environment
#SBATCH --mem=30000                            # server memory requested (per node)
#SBATCH -t 300:00:00                           # Time limit (hh:mm:ss)
#SBATCH --gres=gpu:a6000:8

# >>> conda initialize >>>
declare CONDA_PATH=/share/apps/anaconda3/2021.05/
# !! Contents within this block are managed by 'conda init' !!
__conda_setup="$($CONDA_PATH'/bin/conda' 'shell.bash' 'hook' 2> /dev/null)"
if [ $? -eq 0 ]; then
    eval "$__conda_setup"
else
    if [ -f "$CONDA_PATH/etc/profile.d/conda.sh" ]; then
        . "$CONDA_PATH/etc/profile.d/conda.sh"
    else
        export PATH="$CONDA_PATH/bin:$PATH"
    fi
fi
unset __conda_setup
conda deactivate
conda activate llama_factory
export NCCL_P2P_LEVEL=NVL
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 FORCE_TORCHRUN=1 llamafactory-cli train /home/kh844/Fine-tune/LLaMA-Factory/examples/train_lora/llama31_lora_sft_all.yaml
